# Import necessary libraries
import os
import numpy as np
import pickle
from tqdm import tqdm
from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add, Bidirectional, Attention
from tensorflow.keras.utils import to_categorical, plot_model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau
from nltk.tokenize import word_tokenize
import nltk

nltk.download('punkt')

# ... [Previous constants and functions remain the same] ...

# Modified tokenization method using NLTK
def create_tokenizer(mapping):
    all_captions = []
    for key in mapping:
        for caption in mapping[key]:
            all_captions.append(caption)
    
    # Use NLTK for tokenization
    all_words = [word.lower() for caption in all_captions for word in word_tokenize(caption)]
    word_counts = nltk.FreqDist(all_words)
    vocab = [word for word, count in word_counts.items() if count > 1]  # Remove words that appear only once
    
    word_to_index = {word: i+1 for i, word in enumerate(vocab)}
    word_to_index['<unk>'] = 0  # Add unknown token
    
    return word_to_index

# Modified data generator to use the new tokenization
def data_generator(data_keys, mapping, features, word_to_index, max_length, vocab_size, batch_size):
    while True:
        for key in data_keys:
            captions = mapping[key]
            for caption in captions:
                seq = [word_to_index.get(word, word_to_index['<unk>']) for word in word_tokenize(caption.lower())]
                for i in range(1, len(seq)):
                    in_seq, out_seq = seq[:i], seq[i]
                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]
                    yield [[features[key][0], in_seq], out_seq]

# Modified model architecture
def define_model(vocab_size, max_length):
    # Image feature extractor
    inputs1 = Input(shape=(4096,))
    fe1 = Dropout(0.4)(inputs1)
    fe2 = Dense(256, activation='relu')(fe1)
    
    # Sequence model
    inputs2 = Input(shape=(max_length,))
    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
    se2 = Dropout(0.4)(se1)
    se3 = Bidirectional(LSTM(256, return_sequences=True))(se2)
    
    # Attention mechanism
    attention = Attention()([se3, fe2])
    
    # Decoder
    decoder1 = Dense(256, activation='relu')(attention)
    outputs = Dense(vocab_size, activation='softmax')(decoder1)
    
    # Tie it together
    model = Model(inputs=[inputs1, inputs2], outputs=outputs)
    
    # Compile
    optimizer = Adam(learning_rate=0.001)
    model.compile(loss='categorical_crossentropy', optimizer=optimizer)
    
    print(model.summary())
    plot_model(model, to_file='model.png', show_shapes=True)
    return model

# Modified training function with new hyperparameters and callbacks
def train_model(model, generator, steps, epochs, batch_size):
    callbacks = [
        ReduceLROnPlateau(monitor='loss', factor=0.1, patience=5, min_lr=1e-6)
    ]
    
    model.fit(
        generator,
        epochs=epochs,
        steps_per_epoch=steps // batch_size,
        callbacks=callbacks,
        verbose=1
    )

# Main execution
if __name__ == "__main__":
    # Load captions and extract features
    captions_mapping = load_captions(CAPTIONS_FILE)
    clean_captions(captions_mapping)
    features = extract_features(IMAGES_DIR)

    # Prepare tokenizer
    word_to_index = create_tokenizer(captions_mapping)
    vocab_size = len(word_to_index)
    max_length = max(len(word_tokenize(cap)) for caps in captions_mapping.values())

    # Save features
    with open(FEATURES_FILE, 'wb') as f:
        pickle.dump(features, f)

    # Prepare data generator
    data_keys = list(features.keys())
    steps = len(data_keys)
    batch_size = 64  # Increased batch size
    generator = data_generator(data_keys, captions_mapping, features, word_to_index, max_length, vocab_size, batch_size)

    # Define and compile the model
    model = define_model(vocab_size, max_length)

    # Train the model
    train_model(model, generator, steps, epochs=20, batch_size=batch_size)  # Increased epochs

    # Save the model
    model.save(MODEL_FILE)